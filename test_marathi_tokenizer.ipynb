{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c315c5f",
   "metadata": {},
   "source": [
    "# Marathi BPE Tokenizer Test Notebook\n",
    "\n",
    "This notebook demonstrates how to encode and decode custom Marathi text using the trained Byte-Pair Encoding (BPE) tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0e3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4863bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained Marathi BPE tokenizer\n",
    "TOKENIZER_DIR = \"data/marathi_bpe_tokenizer\"\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.join(TOKENIZER_DIR, \"vocab.json\"),\n",
    "    os.path.join(TOKENIZER_DIR, \"merges.txt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4763d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: माझं नाव गीता आहे.\n",
      "Token IDs: [161, 102, 111, 161, 102, 127, 161, 102, 256, 161, 102, 229, 225, 161, 102, 106, 161, 102, 127, 161, 102, 118, 225, 161, 102, 250, 161, 103, 227, 161, 102, 102, 161, 102, 127, 225, 161, 102, 233, 161, 102, 122, 161, 103, 234, 18]\n",
      "Tokens: ['à', '¤', '®', 'à', '¤', '¾', 'à', '¤', 'Ŀ', 'à', '¤', 'Ĥ', 'Ġ', 'à', '¤', '¨', 'à', '¤', '¾', 'à', '¤', 'µ', 'Ġ', 'à', '¤', 'Ĺ', 'à', '¥', 'Ģ', 'à', '¤', '¤', 'à', '¤', '¾', 'Ġ', 'à', '¤', 'Ĩ', 'à', '¤', '¹', 'à', '¥', 'ĩ', '.']\n"
     ]
    }
   ],
   "source": [
    "# Encode custom Marathi text\n",
    "marathi_text = \"माझं नाव गीता आहे.\"\n",
    "encoded = tokenizer.encode(marathi_text)\n",
    "print(\"Original text:\", marathi_text)\n",
    "print(\"Token IDs:\", encoded.ids)\n",
    "print(\"Tokens:\", encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "498ceca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: माझं नाव गीता आहे.\n"
     ]
    }
   ],
   "source": [
    "# Decode the token IDs back to text\n",
    "decoded = tokenizer.decode(encoded.ids)\n",
    "print(\"Decoded text:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab8d0ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: माझं नाव गीता आहे.\n",
      "Token IDs: [161, 102, 111, 161, 102, 127, 161, 102, 256, 161, 102, 229, 225, 161, 102, 106, 161, 102, 127, 161, 102, 118, 225, 161, 102, 250, 161, 103, 227, 161, 102, 102, 161, 102, 127, 225, 161, 102, 233, 161, 102, 122, 161, 103, 234, 18]\n",
      "Decoded: माझं नाव गीता आहे.\n",
      "-\n",
      "Original: आज हवामान छान आहे.\n",
      "Token IDs: [161, 102, 233, 161, 102, 255, 225, 161, 102, 122, 161, 102, 118, 161, 102, 127, 161, 102, 111, 161, 102, 127, 161, 102, 106, 225, 161, 102, 254, 161, 102, 127, 161, 102, 106, 225, 161, 102, 233, 161, 102, 122, 161, 103, 234, 18]\n",
      "Decoded: आज हवामान छान आहे.\n",
      "-\n",
      "Original: तुम्ही कसे आहात?\n",
      "Token IDs: [161, 102, 102, 161, 103, 228, 161, 102, 111, 161, 103, 240, 161, 102, 122, 161, 103, 227, 225, 161, 102, 248, 161, 102, 121, 161, 103, 234, 225, 161, 102, 233, 161, 102, 122, 161, 102, 127, 161, 102, 102, 35]\n",
      "Decoded: तुम्ही कसे आहात?\n",
      "-\n",
      "Original: पुस्तक वाचा आणि शिका.\n",
      "Token IDs: [161, 102, 108, 161, 103, 228, 161, 102, 121, 161, 103, 240, 161, 102, 102, 161, 102, 248, 225, 161, 102, 118, 161, 102, 127, 161, 102, 253, 161, 102, 127, 225, 161, 102, 233, 161, 102, 101, 161, 102, 128, 225, 161, 102, 119, 161, 102, 128, 161, 102, 248, 161, 102, 127, 18]\n",
      "Decoded: पुस्तक वाचा आणि शिका.\n",
      "-\n",
      "Original: कामाचे तास 9 ऐवजी 12 तास केल्यानं कामगारांचा फायदा की शोषण? तज्ज्ञांचे मत काय?\n",
      "Token IDs: [161, 102, 248, 161, 102, 127, 161, 102, 111, 161, 102, 127, 161, 102, 253, 161, 103, 234, 225, 161, 102, 102, 161, 102, 127, 161, 102, 121, 225, 29, 225, 161, 102, 243, 161, 102, 118, 161, 102, 255, 161, 103, 227, 225, 21, 22, 225, 161, 102, 102, 161, 102, 127, 161, 102, 121, 225, 161, 102, 248, 161, 103, 234, 161, 102, 115, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 106, 161, 102, 229, 225, 161, 102, 248, 161, 102, 127, 161, 102, 111, 161, 102, 250, 161, 102, 127, 161, 102, 113, 161, 102, 127, 161, 102, 229, 161, 102, 253, 161, 102, 127, 225, 161, 102, 109, 161, 102, 127, 161, 102, 112, 161, 102, 104, 161, 102, 127, 225, 161, 102, 248, 161, 103, 227, 225, 161, 102, 119, 161, 103, 238, 161, 102, 120, 161, 102, 101, 35, 225, 161, 102, 102, 161, 102, 255, 161, 103, 240, 161, 102, 255, 161, 103, 240, 161, 102, 257, 161, 102, 127, 161, 102, 229, 161, 102, 253, 161, 103, 234, 225, 161, 102, 111, 161, 102, 102, 225, 161, 102, 248, 161, 102, 127, 161, 102, 112, 35]\n",
      "Decoded: कामाचे तास 9 ऐवजी 12 तास केल्यानं कामगारांचा फायदा की शोषण? तज्ज्ञांचे मत काय?\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple Marathi sentences\n",
    "examples = [\n",
    "    \"माझं नाव गीता आहे.\",\n",
    "    \"आज हवामान छान आहे.\",\n",
    "    \"तुम्ही कसे आहात?\",\n",
    "    \"पुस्तक वाचा आणि शिका.\",\n",
    "    \"कामाचे तास 9 ऐवजी 12 तास केल्यानं कामगारांचा फायदा की शोषण? तज्ज्ञांचे मत काय?\"\n",
    "]\n",
    "for text in examples:\n",
    "    encoded = tokenizer.encode(text)\n",
    "    decoded = tokenizer.decode(encoded.ids)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Token IDs: {encoded.ids}\")\n",
    "    print(f\"Decoded: {decoded}\")\n",
    "    print(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67c2e03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: कारखान्यात काम करणाऱ्या कामगारांच्या कामाच्या वेळेची मर्यादा आता दिवसाला 9 तासांवरुन 12 तास करण्याच्या तरतुदीला राज्य सरकारकडून मान्यता देण्यात आली आहे.\n",
      "\n",
      "राज्य मंत्रिमंडळाच्या बैठकीत यासंदर्भात निर्णय घेण्यात आला असून, 'कारखाने अधिनियम, 1948' मधील काही तरतुदींमध्ये दुरुस्ती करण्यास मान्यता देण्यात आली आहे.\n",
      "\n",
      "मंत्रिमंडळाच्या बैठकीत हा निर्णय घेण्यात आल्याचे जाहीर झाल्यानंतर या निर्णयावर टीकेची झोड उठली आहे. त्यानंतर सरकारनंही त्यावर स्पष्टीकरण दिलं आहे.\n",
      "\n",
      "नेमका हा निर्णय काय आहे? कायद्यातील कोणत्या तरतुदींमध्ये काय बदल करण्यात आले आहेत?\n",
      "\n",
      "त्यांचा नेमका अन्वयार्थ काय आहे? तसेच, या निर्णयाचे कामगारांच्या शारीरिक, मानसिक, आर्थिक बाबींध्ये काय परिणाम होतील, ते पाहूयात.\n",
      "Token IDs: [161, 102, 248, 161, 102, 127, 161, 102, 113, 161, 102, 249, 161, 102, 127, 161, 102, 106, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 102, 225, 161, 102, 248, 161, 102, 127, 161, 102, 111, 225, 161, 102, 248, 161, 102, 113, 161, 102, 101, 161, 102, 127, 161, 102, 113, 161, 102, 125, 161, 103, 240, 161, 102, 112, 161, 102, 127, 225, 161, 102, 248, 161, 102, 127, 161, 102, 111, 161, 102, 250, 161, 102, 127, 161, 102, 113, 161, 102, 127, 161, 102, 229, 161, 102, 253, 161, 103, 240, 161, 102, 112, 161, 102, 127, 225, 161, 102, 248, 161, 102, 127, 161, 102, 111, 161, 102, 127, 161, 102, 253, 161, 103, 240, 161, 102, 112, 161, 102, 127, 225, 161, 102, 118, 161, 103, 234, 161, 102, 116, 161, 103, 234, 161, 102, 253, 161, 103, 227, 225, 161, 102, 111, 161, 102, 113, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 104, 161, 102, 127, 225, 161, 102, 233, 161, 102, 102, 161, 102, 127, 225, 161, 102, 104, 161, 102, 128, 161, 102, 118, 161, 102, 121, 161, 102, 127, 161, 102, 115, 161, 102, 127, 225, 29, 225, 161, 102, 102, 161, 102, 127, 161, 102, 121, 161, 102, 127, 161, 102, 229, 161, 102, 118, 161, 102, 113, 161, 103, 228, 161, 102, 106, 225, 21, 22, 225, 161, 102, 102, 161, 102, 127, 161, 102, 121, 225, 161, 102, 248, 161, 102, 113, 161, 102, 101, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 253, 161, 103, 240, 161, 102, 112, 161, 102, 127, 225, 161, 102, 102, 161, 102, 113, 161, 102, 102, 161, 103, 228, 161, 102, 104, 161, 103, 227, 161, 102, 115, 161, 102, 127, 225, 161, 102, 113, 161, 102, 127, 161, 102, 255, 161, 103, 240, 161, 102, 112, 225, 161, 102, 121, 161, 102, 113, 161, 102, 248, 161, 102, 127, 161, 102, 113, 161, 102, 248, 161, 102, 99, 161, 103, 229, 161, 102, 106, 225, 161, 102, 111, 161, 102, 127, 161, 102, 106, 161, 103, 240, 161, 102, 112, 161, 102, 102, 161, 102, 127, 225, 161, 102, 104, 161, 103, 234, 161, 102, 101, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 102, 225, 161, 102, 233, 161, 102, 115, 161, 103, 227, 225, 161, 102, 233, 161, 102, 122, 161, 103, 234, 18, 203, 203, 161, 102, 113, 161, 102, 127, 161, 102, 255, 161, 103, 240, 161, 102, 112, 225, 161, 102, 111, 161, 102, 229, 161, 102, 102, 161, 103, 240, 161, 102, 113, 161, 102, 128, 161, 102, 111, 161, 102, 229, 161, 102, 99, 161, 102, 116, 161, 102, 127, 161, 102, 253, 161, 103, 240, 161, 102, 112, 161, 102, 127, 225, 161, 102, 110, 161, 103, 235, 161, 102, 259, 161, 102, 248, 161, 103, 227, 161, 102, 102, 225, 161, 102, 112, 161, 102, 127, 161, 102, 121, 161, 102, 229, 161, 102, 104, 161, 102, 113, 161, 103, 240, 161, 102, 260, 161, 102, 127, 161, 102, 102, 225, 161, 102, 106, 161, 102, 128, 161, 102, 113, 161, 103, 240, 161, 102, 101, 161, 102, 112, 225, 161, 102, 251, 161, 103, 234, 161, 102, 101, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 102, 225, 161, 102, 233, 161, 102, 115, 161, 102, 127, 225, 161, 102, 232, 161, 102, 121, 161, 103, 229, 161, 102, 106, 16, 225, 11, 161, 102, 248, 161, 102, 127, 161, 102, 113, 161, 102, 249, 161, 102, 127, 161, 102, 106, 161, 103, 234, 225, 161, 102, 232, 161, 102, 105, 161, 102, 128, 161, 102, 106, 161, 102, 128, 161, 102, 112, 161, 102, 111, 16, 225, 21, 29, 24, 28, 11, 225, 161, 102, 111, 161, 102, 105, 161, 103, 227, 161, 102, 115, 225, 161, 102, 248, 161, 102, 127, 161, 102, 122, 161, 103, 227, 225, 161, 102, 102, 161, 102, 113, 161, 102, 102, 161, 103, 228, 161, 102, 104, 161, 103, 227, 161, 102, 229, 161, 102, 111, 161, 102, 105, 161, 103, 240, 161, 102, 112, 161, 103, 234, 225, 161, 102, 104, 161, 103, 228, 161, 102, 113, 161, 103, 228, 161, 102, 121, 161, 103, 240, 161, 102, 102, 161, 103, 227, 225, 161, 102, 248, 161, 102, 113, 161, 102, 101, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 121, 225, 161, 102, 111, 161, 102, 127, 161, 102, 106, 161, 103, 240, 161, 102, 112, 161, 102, 102, 161, 102, 127, 225, 161, 102, 104, 161, 103, 234, 161, 102, 101, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 102, 225, 161, 102, 233, 161, 102, 115, 161, 103, 227, 225, 161, 102, 233, 161, 102, 122, 161, 103, 234, 18, 203, 203, 161, 102, 111, 161, 102, 229, 161, 102, 102, 161, 103, 240, 161, 102, 113, 161, 102, 128, 161, 102, 111, 161, 102, 229, 161, 102, 99, 161, 102, 116, 161, 102, 127, 161, 102, 253, 161, 103, 240, 161, 102, 112, 161, 102, 127, 225, 161, 102, 110, 161, 103, 235, 161, 102, 259, 161, 102, 248, 161, 103, 227, 161, 102, 102, 225, 161, 102, 122, 161, 102, 127, 225, 161, 102, 106, 161, 102, 128, 161, 102, 113, 161, 103, 240, 161, 102, 101, 161, 102, 112, 225, 161, 102, 251, 161, 103, 234, 161, 102, 101, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 102, 225, 161, 102, 233, 161, 102, 115, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 253, 161, 103, 234, 225, 161, 102, 255, 161, 102, 127, 161, 102, 122, 161, 103, 227, 161, 102, 113, 225, 161, 102, 256, 161, 102, 127, 161, 102, 115, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 106, 161, 102, 229, 161, 102, 102, 161, 102, 113, 225, 161, 102, 112, 161, 102, 127, 225, 161, 102, 106, 161, 102, 128, 161, 102, 113, 161, 103, 240, 161, 102, 101, 161, 102, 112, 161, 102, 127, 161, 102, 118, 161, 102, 113, 225, 161, 102, 258, 161, 103, 227, 161, 102, 248, 161, 103, 234, 161, 102, 253, 161, 103, 227, 225, 161, 102, 256, 161, 103, 238, 161, 102, 99, 225, 161, 102, 236, 161, 102, 259, 161, 102, 115, 161, 103, 227, 225, 161, 102, 233, 161, 102, 122, 161, 103, 234, 18, 225, 161, 102, 102, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 106, 161, 102, 229, 161, 102, 102, 161, 102, 113, 225, 161, 102, 121, 161, 102, 113, 161, 102, 248, 161, 102, 127, 161, 102, 113, 161, 102, 106, 161, 102, 229, 161, 102, 122, 161, 103, 227, 225, 161, 102, 102, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 118, 161, 102, 113, 225, 161, 102, 121, 161, 103, 240, 161, 102, 108, 161, 102, 120, 161, 103, 240, 161, 102, 258, 161, 103, 227, 161, 102, 248, 161, 102, 113, 161, 102, 101, 225, 161, 102, 104, 161, 102, 128, 161, 102, 115, 161, 102, 229, 225, 161, 102, 233, 161, 102, 122, 161, 103, 234, 18, 203, 203, 161, 102, 106, 161, 103, 234, 161, 102, 111, 161, 102, 248, 161, 102, 127, 225, 161, 102, 122, 161, 102, 127, 225, 161, 102, 106, 161, 102, 128, 161, 102, 113, 161, 103, 240, 161, 102, 101, 161, 102, 112, 225, 161, 102, 248, 161, 102, 127, 161, 102, 112, 225, 161, 102, 233, 161, 102, 122, 161, 103, 234, 35, 225, 161, 102, 248, 161, 102, 127, 161, 102, 112, 161, 102, 104, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 102, 161, 103, 227, 161, 102, 115, 225, 161, 102, 248, 161, 103, 238, 161, 102, 101, 161, 102, 102, 161, 103, 240, 161, 102, 112, 161, 102, 127, 225, 161, 102, 102, 161, 102, 113, 161, 102, 102, 161, 103, 228, 161, 102, 104, 161, 103, 227, 161, 102, 229, 161, 102, 111, 161, 102, 105, 161, 103, 240, 161, 102, 112, 161, 103, 234, 225, 161, 102, 248, 161, 102, 127, 161, 102, 112, 225, 161, 102, 110, 161, 102, 104, 161, 102, 115, 225, 161, 102, 248, 161, 102, 113, 161, 102, 101, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 102, 225, 161, 102, 233, 161, 102, 115, 161, 103, 234, 225, 161, 102, 233, 161, 102, 122, 161, 103, 234, 161, 102, 102, 35, 203, 203, 161, 102, 102, 161, 103, 240, 161, 102, 112, 161, 102, 127, 161, 102, 229, 161, 102, 253, 161, 102, 127, 225, 161, 102, 106, 161, 103, 234, 161, 102, 111, 161, 102, 248, 161, 102, 127, 225, 161, 102, 232, 161, 102, 106, 161, 103, 240, 161, 102, 118, 161, 102, 112, 161, 102, 127, 161, 102, 113, 161, 103, 240, 161, 102, 103, 225, 161, 102, 248, 161, 102, 127, 161, 102, 112, 225, 161, 102, 233, 161, 102, 122, 161, 103, 234, 35, 225, 161, 102, 102, 161, 102, 121, 161, 103, 234, 161, 102, 253, 16, 225, 161, 102, 112, 161, 102, 127, 225, 161, 102, 106, 161, 102, 128, 161, 102, 113, 161, 103, 240, 161, 102, 101, 161, 102, 112, 161, 102, 127, 161, 102, 253, 161, 103, 234, 225, 161, 102, 248, 161, 102, 127, 161, 102, 111, 161, 102, 250, 161, 102, 127, 161, 102, 113, 161, 102, 127, 161, 102, 229, 161, 102, 253, 161, 103, 240, 161, 102, 112, 161, 102, 127, 225, 161, 102, 119, 161, 102, 127, 161, 102, 113, 161, 103, 227, 161, 102, 113, 161, 102, 128, 161, 102, 248, 16, 225, 161, 102, 111, 161, 102, 127, 161, 102, 106, 161, 102, 121, 161, 102, 128, 161, 102, 248, 16, 225, 161, 102, 233, 161, 102, 113, 161, 103, 240, 161, 102, 103, 161, 102, 128, 161, 102, 248, 225, 161, 102, 110, 161, 102, 127, 161, 102, 110, 161, 103, 227, 161, 102, 229, 161, 102, 105, 161, 103, 240, 161, 102, 112, 161, 103, 234, 225, 161, 102, 248, 161, 102, 127, 161, 102, 112, 225, 161, 102, 108, 161, 102, 113, 161, 102, 128, 161, 102, 101, 161, 102, 127, 161, 102, 111, 225, 161, 102, 122, 161, 103, 238, 161, 102, 102, 161, 103, 227, 161, 102, 115, 16, 225, 161, 102, 102, 161, 103, 234, 225, 161, 102, 108, 161, 102, 127, 161, 102, 122, 161, 103, 229, 161, 102, 112, 161, 102, 127, 161, 102, 102, 18]\n",
      "Decoded: कारखान्यात काम करणाऱ्या कामगारांच्या कामाच्या वेळेची मर्यादा आता दिवसाला 9 तासांवरुन 12 तास करण्याच्या तरतुदीला राज्य सरकारकडून मान्यता देण्यात आली आहे.\n",
      "\n",
      "राज्य मंत्रिमंडळाच्या बैठकीत यासंदर्भात निर्णय घेण्यात आला असून, 'कारखाने अधिनियम, 1948' मधील काही तरतुदींमध्ये दुरुस्ती करण्यास मान्यता देण्यात आली आहे.\n",
      "\n",
      "मंत्रिमंडळाच्या बैठकीत हा निर्णय घेण्यात आल्याचे जाहीर झाल्यानंतर या निर्णयावर टीकेची झोड उठली आहे. त्यानंतर सरकारनंही त्यावर स्पष्टीकरण दिलं आहे.\n",
      "\n",
      "नेमका हा निर्णय काय आहे? कायद्यातील कोणत्या तरतुदींमध्ये काय बदल करण्यात आले आहेत?\n",
      "\n",
      "त्यांचा नेमका अन्वयार्थ काय आहे? तसेच, या निर्णयाचे कामगारांच्या शारीरिक, मानसिक, आर्थिक बाबींध्ये काय परिणाम होतील, ते पाहूयात.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"कारखान्यात काम करणाऱ्या कामगारांच्या कामाच्या वेळेची मर्यादा आता दिवसाला 9 तासांवरुन 12 तास करण्याच्या तरतुदीला राज्य सरकारकडून मान्यता देण्यात आली आहे.\n",
    "\n",
    "राज्य मंत्रिमंडळाच्या बैठकीत यासंदर्भात निर्णय घेण्यात आला असून, 'कारखाने अधिनियम, 1948' मधील काही तरतुदींमध्ये दुरुस्ती करण्यास मान्यता देण्यात आली आहे.\n",
    "\n",
    "मंत्रिमंडळाच्या बैठकीत हा निर्णय घेण्यात आल्याचे जाहीर झाल्यानंतर या निर्णयावर टीकेची झोड उठली आहे. त्यानंतर सरकारनंही त्यावर स्पष्टीकरण दिलं आहे.\n",
    "\n",
    "नेमका हा निर्णय काय आहे? कायद्यातील कोणत्या तरतुदींमध्ये काय बदल करण्यात आले आहेत?\n",
    "\n",
    "त्यांचा नेमका अन्वयार्थ काय आहे? तसेच, या निर्णयाचे कामगारांच्या शारीरिक, मानसिक, आर्थिक बाबींध्ये काय परिणाम होतील, ते पाहूयात.\"\"\"\n",
    "\n",
    "encoded = tokenizer.encode(text)\n",
    "decoded = tokenizer.decode(encoded.ids)\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Token IDs: {encoded.ids}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ed03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc65ceaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'marathi_gpt2.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m config = GPT2Config()\n\u001b[32m      7\u001b[39m model = GPT2Model(config)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmarathi_gpt2.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      9\u001b[39m model.eval()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Accumulate token counts\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/BolBot/.venv/lib/python3.12/site-packages/torch/serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/BolBot/.venv/lib/python3.12/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/BolBot/.venv/lib/python3.12/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'marathi_gpt2.pt'"
     ]
    }
   ],
   "source": [
    "# Read the entire tokenized dataset and compute output token distributions\n",
    "import torch\n",
    "from marathi_gpt2 import GPT2Config, GPT2Model\n",
    "\n",
    "data_path = \"data/mrwiki_text_tokenized.txt\"\n",
    "config = GPT2Config()\n",
    "model = GPT2Model(config)\n",
    "model.load_state_dict(torch.load(\"marathi_gpt2.pt\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# Accumulate token counts\n",
    "from collections import Counter\n",
    "output_token_counts = Counter()\n",
    "\n",
    "def get_batches(file_path, batch_size=32, seq_len=64):\n",
    "    batch = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            ids = [int(tok) for tok in line.strip().split()]\n",
    "            for i in range(0, len(ids) - seq_len, seq_len):\n",
    "                batch.append(ids[i:i+seq_len])\n",
    "                if len(batch) == batch_size:\n",
    "                    yield torch.tensor(batch, dtype=torch.long)\n",
    "                    batch = []\n",
    "    if batch:\n",
    "        yield torch.tensor(batch, dtype=torch.long)\n",
    "\n",
    "# Go through the dataset and accumulate output token distributions\n",
    "for batch in get_batches(data_path, batch_size=8, seq_len=64):\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        # Get most probable token at each position\n",
    "        top_tokens = torch.argmax(probs, dim=-1)\n",
    "        output_token_counts.update(top_tokens.flatten().tolist())\n",
    "\n",
    "print(\"Top 20 most frequent output tokens:\")\n",
    "for token_id, count in output_token_counts.most_common(20):\n",
    "    print(f\"Token ID: {token_id}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a694e8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent tokens in the dataset:\n",
      "Token ID: 161, Count: 112728934\n",
      "Token ID: 102, Count: 89546448\n",
      "Token ID: 103, Count: 28317864\n",
      "Token ID: 225, Count: 21577982\n",
      "Token ID: 127, Count: 12298955\n",
      "Token ID: 240, Count: 8723177\n",
      "Token ID: 113, Count: 7302094\n",
      "Token ID: 234, Count: 5654511\n",
      "Token ID: 106, Count: 4722671\n",
      "Token ID: 112, Count: 4476277\n",
      "Token ID: 227, Count: 4268463\n",
      "Token ID: 248, Count: 4216559\n",
      "Token ID: 121, Count: 4029293\n",
      "Token ID: 128, Count: 3955339\n",
      "Token ID: 115, Count: 3724003\n",
      "Token ID: 118, Count: 3693601\n",
      "Token ID: 229, Count: 3644862\n",
      "Token ID: 65, Count: 3542081\n",
      "Token ID: 96, Count: 3541505\n",
      "Token ID: 63, Count: 3540951\n"
     ]
    }
   ],
   "source": [
    "# Compute frequency of tokens in the tokenized dataset\n",
    "from collections import Counter\n",
    "\n",
    "data_path = \"data/mrwiki_text_tokenized.txt\"\n",
    "token_counts = Counter()\n",
    "\n",
    "with open(data_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        ids = [int(tok) for tok in line.strip().split()]\n",
    "        token_counts.update(ids)\n",
    "\n",
    "print(\"Top 20 most frequent tokens in the dataset:\")\n",
    "for token_id, count in token_counts.most_common(20):\n",
    "    print(f\"Token ID: {token_id}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a5c53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded tokens for the top 20 most frequent token IDs:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tokenizers.decoders.ByteLevel' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDecoded tokens for the top 20 most frequent token IDs:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token_id, count \u001b[38;5;129;01min\u001b[39;00m top_20:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     token_str = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(token_id, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m token_str \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      7\u001b[39m         \u001b[38;5;66;03m# Try using the tokenizer's id_to_token method if available\u001b[39;00m\n\u001b[32m      8\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'tokenizers.decoders.ByteLevel' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# Decode and print the top 20 most frequent tokens in the dataset\n",
    "top_20 = token_counts.most_common(20)\n",
    "print(\"\\nDecoded tokens for the top 20 most frequent token IDs:\")\n",
    "for token_id, count in top_20:\n",
    "    try:\n",
    "        # Try using the id_to_token method (if available)\n",
    "        token_str = tokenizer.id_to_token(token_id)\n",
    "    except AttributeError:\n",
    "        # Fallback: use the vocabulary if available\n",
    "        if hasattr(tokenizer, 'vocab') and isinstance(tokenizer.vocab, dict):\n",
    "            # Find the token string by id\n",
    "            inv_vocab = {v: k for k, v in tokenizer.vocab.items()}\n",
    "            token_str = inv_vocab.get(token_id, '<UNK>')\n",
    "        else:\n",
    "            token_str = '<UNK>'\n",
    "    print(f\"Token ID: {token_id}, Count: {count}, Token: {repr(token_str)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
